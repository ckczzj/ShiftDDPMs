train_dataset_config:
  name: "CIFAR"
  data_path: "./data/cifar"
  image_size: &image_size 32
  image_channel: &image_channel 3
  num_class: &num_class 10
  train: True
eval_dataset_config:
  train: False

diffusion_config:
  timesteps: 1000
  betas_type: "linear"
  shift_type: "quadratic_shift"

denoise_fn_config:
  model: "CIFARDenoiseFn"
  # num_class: *num_class  # uncomment this line for conditional training
  dims: 2
  input_channel: *image_channel
  base_channel: 128
  channel_multiplier: [ 1, 2, 2, 2 ] # there are len(channel_multiplier) blocks, downsample and upsample len(channel_multiplier)-1 times, no downsample or upsample at the last multiplier
  num_residual_blocks_of_a_block: 2
  dropout: 0.0

  # below settings are useful only when the attention_resolutions list is not empty
  attention_resolutions: [ 2 ] # attention will be used at x-times downsampling if x in this list, empty for not using attention
  use_new_attention_order: False  # True for MNIST and False for the others
  num_heads: 4
  head_channel: -1 # if head_channel==-1 then num_heads=num_heads, else num_heads=(multiplier*base_channel)//head_channel

shift_predictor_config:
  model: "CIFARShiftPredictor"
  num_class: *num_class
  image_size: *image_size
  image_channel: *image_channel

dataloader_config:
  train:
    num_workers: 4
    batch_size: 32 # batch_size for each process
  eval:
    num_generations: 36 # showing a 6x6 image grid in tensorboard

optimizer_config:
  lr: 1e-4
  adam_betas: (0.9, 0.999)
  adam_eps: 1e-8
  weight_decay: 0.0
  enable_amp: False

runner_config:
  display_steps: 100
  evaluate_every_steps: 5000
  save_latest_every_steps: 1000
  save_checkpoint_every_steps: 5000
  num_iterations: 1
  ema_every: 1
  ema_decay: 0.9999
  compile: False